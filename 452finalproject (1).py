# -*- coding: utf-8 -*-
"""452FinalProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-fZ4s4XikuVqGrFLXAlVEcYZueSc1nXm
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, models
import matplotlib.pyplot as plt
import numpy as np

# Data transformations
transform_train = transforms.Compose([
    transforms.RandomHorizontalFlip(),  # Data augmentation
    transforms.RandomCrop(32, padding=4),  # Data augmentation
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# Download CIFAR-10 dataset
train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)

# Data loaders
train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)

class BaselineCNN(nn.Module):
    def __init__(self):
        super(BaselineCNN, self).__init__()
        self.conv_layers = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )
        self.fc_layers = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64 * 8 * 8, 256),
            nn.ReLU(),
            nn.Dropout(0.5),  # Regularization
            nn.Linear(256, 10)
        )

    def forward(self, x):
        x = self.conv_layers(x)
        x = self.fc_layers(x)
        return x

def train(model, criterion, optimizer, loader, device):
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    for inputs, targets in loader:
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()
    return total_loss / len(loader), correct / total

from sklearn.metrics import classification_report


def test_with_report(model, criterion, loader, device, class_names):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    all_preds = []
    all_targets = []

    with torch.no_grad():
        for inputs, targets in loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)

            total_loss += loss.item()
            _, predicted = outputs.max(1)

            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

            all_preds.extend(predicted.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())

    avg_loss = total_loss / len(loader)
    accuracy = correct / total

    # Generate and print classification report
    report = classification_report(all_targets, all_preds, target_names=class_names)
    print("Classification Report:\n")
    print(report)

    return avg_loss, accuracy

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = BaselineCNN().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

num_epochs = 20
train_losses, train_accuracies = [], []
test_losses, test_accuracies = [], []

class_names = [
    'airplane', 'automobile', 'bird', 'cat',
    'deer', 'dog', 'frog', 'horse', 'ship', 'truck'
]

for epoch in range(num_epochs):
    train_loss, train_acc = train(model, criterion, optimizer, train_loader, device)
    test_loss, test_acc = test_with_report(model, criterion, test_loader, device, class_names)

    train_losses.append(train_loss)
    train_accuracies.append(train_acc)
    test_losses.append(test_loss)
    test_accuracies.append(test_acc)

    print(f"Epoch {epoch+1}/{num_epochs} - "
          f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} - "
          f"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}")

# Plot accuracy curves
plt.plot(train_accuracies, label='Baseline Train Accuracy')
plt.plot(test_accuracies, label='Baseline Test Accuracy')

# print(classification_report(y_test, y_pred))
# # plt.plot(resnet_train_accuracies, label='ResNet Train Accuracy')
# # plt.plot(resnet_test_accuracies, label='ResNet Test Accuracy')
# # Accuracy, precision, recall, F1-score, and loss curves.
plt.legend()
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Model Accuracy')
plt.show()

print(train_losses)

plt.plot(train_losses, label='Baseline Train loss')
plt.plot(test_losses, label='Baseline Test loss')
# plt.plot(test_accuracies, label='Baseline Test Accuracy')

# print(classification_report(y_test, y_pred))
# # plt.plot(resnet_train_accuracies, label='ResNet Train Accuracy')
# # plt.plot(resnet_test_accuracies, label='ResNet Test Accuracy')
# # Accuracy, precision, recall, F1-score, and loss curves.
plt.legend()
plt.xlabel('Epochs')
plt.ylabel('loss')
plt.title('Model loss')
plt.show()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, models
from sklearn.metrics import classification_report

# Define CIFAR-10 class names
class_names = [
    'airplane', 'automobile', 'bird', 'cat',
    'deer', 'dog', 'frog', 'horse', 'ship', 'truck'
]

#1. Data Preparation
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),  # Data augmentation
    transforms.RandomCrop(32, padding=4),  # Data augmentation
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),
])

#
train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# 2. Define ResNet Model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

resnet_model = models.resnet18(pretrained=True)
resnet_model.fc = nn.Linear(resnet_model.fc.in_features, 10)  # Adjust output layer for CIFAR-10
resnet_model = resnet_model.to(device)

num_epochs = 5
train_losses, train_accuracies = [], []
test_losses, test_accuracies = [], []

for epoch in range(num_epochs):
    train_loss, train_acc = train(resnet_model, criterion, optimizer, train_loader, device)
    test_loss, test_acc = test_with_report(resnet_model, criterion, test_loader, device, class_names)

    train_losses.append(train_loss)
    train_accuracies.append(train_acc)
    test_losses.append(test_loss)
    test_accuracies.append(test_acc)

    print(f"Epoch {epoch+1}/{num_epochs} - "
          f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} - "
          f"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}")

print(train_losses)

plt.plot(train_losses, label='Baseline Train loss')
plt.plot(test_losses, label='Baseline Test loss')
# plt.plot(test_accuracies, label='Baseline Test Accuracy')

# print(classification_report(y_test, y_pred))
# # plt.plot(resnet_train_accuracies, label='ResNet Train Accuracy')
# # plt.plot(resnet_test_accuracies, label='ResNet Test Accuracy')
# # Accuracy, precision, recall, F1-score, and loss curves.
plt.legend()
plt.xlabel('Epochs')
plt.ylabel('loss')
plt.title('Model loss')
plt.show()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, models
from sklearn.metrics import classification_report

# Define CIFAR-10 class names
class_names = [
    'airplane', 'automobile', 'bird', 'cat',
    'deer', 'dog', 'frog', 'horse', 'ship', 'truck'
]

# 1. Data Preparation
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),  # Data augmentation
    transforms.RandomCrop(32, padding=4),  # Data augmentation
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),
])

# Load CIFAR-10 Dataset
train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# 2. Define DenseNet Model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
densenet_model = models.densenet121(pretrained=True)  # DenseNet-121
densenet_model.classifier = nn.Linear(densenet_model.classifier.in_features, 10)  # Adjust for 10 classes
densenet_model = densenet_model.to(device)

# 3. Define Loss Function and Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(densenet_model.parameters(), lr=0.001)

# 4. Training Function
def train(model, criterion, optimizer, loader, device):
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for inputs, targets in loader:
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()

        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

    avg_loss = total_loss / len(loader)
    accuracy = correct / total
    return avg_loss, accuracy

# 5. Testing Function with Classification Report
def test_with_report(model, criterion, loader, device, class_names):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    all_preds = []
    all_targets = []

    with torch.no_grad():
        for inputs, targets in loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)

            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

            all_preds.extend(predicted.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())

    avg_loss = total_loss / len(loader)
    accuracy = correct / total

    # Print classification report
    report = classification_report(all_targets, all_preds, target_names=class_names)
    print("Classification Report:\n")
    print(report)

    return avg_loss, accuracy

# 6. Training Loop
num_epochs = 10
train_losses, train_accuracies = [], []
test_losses, test_accuracies = [], []

for epoch in range(num_epochs):
    train_loss, train_acc = train(densenet_model, criterion, optimizer, train_loader, device)
    test_loss, test_acc = test_with_report(densenet_model, criterion, test_loader, device, class_names)

    train_losses.append(train_loss)
    train_accuracies.append(train_acc)
    test_losses.append(test_loss)
    test_accuracies.append(test_acc)

    print(f"Epoch {epoch+1}/{num_epochs} - "
          f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} - "
          f"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}")

print(train_losses)

plt.plot(train_losses, label='Baseline Train loss')
plt.plot(test_losses, label='Baseline Test loss')
# plt.plot(test_accuracies, label='Baseline Test Accuracy')

# print(classification_report(y_test, y_pred))
# # plt.plot(resnet_train_accuracies, label='ResNet Train Accuracy')
# # plt.plot(resnet_test_accuracies, label='ResNet Test Accuracy')
# # Accuracy, precision, recall, F1-score, and loss curves.
plt.legend()
plt.xlabel('Epochs')
plt.ylabel('loss')
plt.title('Model loss')
plt.show()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from sklearn.metrics import classification_report

# Define CIFAR-10 class names
class_names = [
    'airplane', 'automobile', 'bird', 'cat',
    'deer', 'dog', 'frog', 'horse', 'ship', 'truck'
]

# 1. Data Preparation
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),  # Data augmentation
    transforms.RandomCrop(32, padding=4),  # Data augmentation
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),
])

# Load CIFAR-10 Dataset
train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# 2. Custom CNN Model with Dropout
class CNNWithDropout(nn.Module):
    def __init__(self):
        super(CNNWithDropout, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout = nn.Dropout(0.5)  # Dropout with a probability of 50%
        self.fc1 = nn.Linear(128 * 4 * 4, 512)  # CIFAR-10 images are 32x32 -> 128x4x4 after pooling
        self.fc2 = nn.Linear(512, 10)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = self.pool(torch.relu(self.conv3(x)))
        x = x.view(-1, 128 * 4 * 4)  # Flatten
        x = self.dropout(torch.relu(self.fc1(x)))  # Apply Dropout
        x = self.fc2(x)
        return x

# Initialize the model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = CNNWithDropout().to(device)

# 3. Define Loss Function and Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 4. Training Function
def train(model, criterion, optimizer, loader, device):
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for inputs, targets in loader:
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()

        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

    avg_loss = total_loss / len(loader)
    accuracy = correct / total
    return avg_loss, accuracy

# 5. Testing Function with Classification Report
def test_with_report(model, criterion, loader, device, class_names):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    all_preds = []
    all_targets = []

    with torch.no_grad():
        for inputs, targets in loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)

            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

            all_preds.extend(predicted.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())

    avg_loss = total_loss / len(loader)
    accuracy = correct / total

    # Print classification report
    report = classification_report(all_targets, all_preds, target_names=class_names)
    print("Classification Report:\n")
    print(report)

    return avg_loss, accuracy

# 6. Training Loop
num_epochs = 10
train_losses, train_accuracies = [], []
test_losses, test_accuracies = [], []

for epoch in range(num_epochs):
    train_loss, train_acc = train(model, criterion, optimizer, train_loader, device)
    test_loss, test_acc = test_with_report(model, criterion, test_loader, device, class_names)

    train_losses.append(train_loss)
    train_accuracies.append(train_acc)
    test_losses.append(test_loss)
    test_accuracies.append(test_acc)

    print(f"Epoch {epoch+1}/{num_epochs} - "
          f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} - "
          f"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}")

print(train_losses)

plt.plot(train_losses, label='Baseline Train loss')
plt.plot(test_losses, label='Baseline Test loss')
# plt.plot(test_accuracies, label='Baseline Test Accuracy')

# print(classification_report(y_test, y_pred))
# # plt.plot(resnet_train_accuracies, label='ResNet Train Accuracy')
# # plt.plot(resnet_test_accuracies, label='ResNet Test Accuracy')
# # Accuracy, precision, recall, F1-score, and loss curves.
plt.legend()
plt.xlabel('Epochs')
plt.ylabel('loss')
plt.title('Model loss')
plt.show()